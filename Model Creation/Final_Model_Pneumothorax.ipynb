{"cells":[{"cell_type":"markdown","metadata":{"id":"ISRJX6k3FG-4"},"source":["# PyTorch Inception and DenseNet\n","\n"]},{"cell_type":"markdown","metadata":{"id":"E1jMZOSrwuPM"},"source":["# Preprocessing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gACgQydzrbzh"},"outputs":[],"source":["!pip install pydicom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ckyZPYj4v2X4"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lszPQTLIFRD3"},"outputs":[],"source":["##### Could clean this up a bit\n","\n","%matplotlib inline\n","\n","import os\n","import time\n","import copy\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import pandas as pd\n","import cv2\n","\n","import torchvision\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","from torchvision.io import read_image\n","from torchvision import datasets, models\n","from torchvision.utils import make_grid\n","from torchvision.utils import save_image\n","from torch.utils import data\n","from torchsummary import summary\n","#import torchmetrics\n","#from torchmetrics.classification import BinaryAUROC\n","\n","from pathlib import Path\n","from pydicom import dcmread\n","from tqdm.notebook import tqdm\n","import skimage.io as sk\n","\n","from PIL import Image\n","\n","#torch.manual_seed(0)\n","\n","print('Using PyTorch version', torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFSZ33GyUAiD"},"outputs":[],"source":["##### Specify Device\n","\n","device = torch.device(\"cpu\")\n","\n","if torch.cuda.is_available():\n","   print(\"Training on GPU\")\n","   device = torch.device(\"cuda:0\")\n"]},{"cell_type":"markdown","metadata":{"id":"HjZ_IAOng1Cv"},"source":["#Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysRMl6Le3or9"},"outputs":[],"source":["#Make Labels\n","df = pd.read_csv(\"/content/drive/MyDrive/Saliency Map Research 2023/Pneumothorax Dataset/Pneumothorax_Labels_Final.csv\")\n","df = df.rename(columns={\"label\": \"Target\", \"ImageId\": \"patientId\"})\n","df2 = df[['Split','patientId','Target']]\n","\n","df_train = df2.loc[(df2['Split'] == 'train')]\n","df_train = df_train.drop(df_train.columns[[0]], axis=1)\n","df_train = df_train.reset_index(drop=True)\n","\n","df_val = df2.loc[(df2['Split'] == 'val')]\n","df_val = df_val.drop(df_val.columns[[0]], axis=1)\n","df_val = df_val.reset_index(drop=True)\n","\n","df_test = df2.loc[(df2['Split'] == 'test')]\n","df_test = df_test.drop(df_test.columns[[0]], axis=1)\n","df_test = df_test.reset_index(drop=True)\n","\n","#Reduce Size for testing\n","#train_total = 200\n","#val_total = 100\n","#test_total = 0\n","\n","#df_train = df_train[:train_total]\n","#df_val = df_val[:val_total]\n","#df_test = df_test[:test_total]\n","\n","#Path Split\n","root = '/content/drive/MyDrive/Saliency Map Research 2023/Pneumothorax Dataset/images_png/PNG/train'\n","\n","train_labels = df_train[['patientId', 'Target']].to_numpy()\n","val_labels = df_val[['patientId', 'Target']].to_numpy()\n","test_labels = df_test[['patientId', 'Target']].to_numpy()\n","\n","train_paths = [os.path.join(root, image[0]) for image in train_labels]\n","val_paths = [os.path.join(root, image[0]) for image in val_labels]\n","test_paths = [os.path.join(root, image[0]) for image in test_labels]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tRakZRm9LbaW"},"outputs":[],"source":["print(f'patientId: {train_labels[0][0]}, Target: {train_labels[0][1]}')\n","print(len(train_paths))\n","print(len(test_paths))\n","df.shape[0]"]},{"cell_type":"markdown","metadata":{"id":"HmtAibVFxdR-"},"source":["# Transformation & Dataloader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzrEDTf9aue1"},"outputs":[],"source":["##### Transformations\n","transform_v3 = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize(size=(299)),\n","    torchvision.transforms.RandomHorizontalFlip(),\n","    torchvision.transforms.ToTensor(),\n","    #torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","transform_dense = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize(size=(224)),\n","    #torchvision.transforms.RandomHorizontalFlip(),\n","    torchvision.transforms.ToTensor(),\n","    #torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","##### Custom Dataset\n","\n","class Dataset(data.Dataset):\n","\n","    def __init__(self, paths, labels, transform=None):\n","        self.paths = paths\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        image = Image.open(f'{self.paths[index]}.png').convert(\"RGB\")\n","        #image = image.pixel_array\n","        #image = image / 255.0\n","\n","        #image = (255*image).clip(0, 255).astype(np.uint8)\n","        #image = Image.fromarray(image).convert('RGB')\n","\n","        label = self.labels[index][1]\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","    def __len__(self):\n","\n","        return len(self.paths)\n","\n","class PneumothoraxDataset(Dataset):\n","    def __init__(self, root_dir, annotation_file, transform=None):\n","        self.root_dir = root_dir\n","        self.annotations = annotation_file\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        img_id = (self.annotations.iloc[index, 0] + '.png')\n","        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n","        y_label = torch.tensor(float(self.annotations.iloc[index, 1]))\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        return (img, y_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mb-xbrnATbCx"},"outputs":[],"source":["##### Dataloader ONLY TRAIN AND VAL RIGHT NOW\n","\n","batch_sz = 4\n","\n","##### INCEPTION\n","train_dataset_v3 = PneumothoraxDataset(root, df_train, transform=transform_v3)\n","val_dataset_v3 = PneumothoraxDataset(root, df_val,transform=transform_v3)\n","train_loader_v3 = torch.utils.data.DataLoader(dataset=train_dataset_v3, batch_size=batch_sz, shuffle=True)\n","val_loader_v3 = torch.utils.data.DataLoader(dataset=val_dataset_v3, batch_size=batch_sz, shuffle=False)\n","\n","print('Number of training batches', len(train_loader_v3))\n","print('Number of validation batches', len(val_loader_v3))\n","\n","##### DENSE\n","train_dataset_dense = PneumothoraxDataset(root, df_train, transform=transform_dense)\n","val_dataset_dense = PneumothoraxDataset(root, df_val, transform=transform_dense)\n","train_loader_dense = torch.utils.data.DataLoader(dataset=train_dataset_dense, batch_size=batch_sz, shuffle=True)\n","val_loader_dense = torch.utils.data.DataLoader(dataset=val_dataset_dense, batch_size=batch_sz, shuffle=False)\n","\n","##### SEPERATING INTO DICTIONARIES FOR EASE OF USE\n","dataloaders_dict_v3 = {'train': train_loader_v3, 'val': val_loader_v3}\n","dataloaders_dict_dense = {'train': train_loader_dense, 'val': val_loader_dense}"]},{"cell_type":"markdown","metadata":{"id":"0hg8DNqXxZsB"},"source":["# Data Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFS7FRMQUPpe"},"outputs":[],"source":["batch = iter(train_loader_v3)\n","images, labels = next(batch)\n","\n","image_grid = torchvision.utils.make_grid(images[:4])\n","image_np = image_grid.numpy()\n","img = np.transpose(image_np, (1, 2, 0))\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"LUQh8pjeZb3b"},"source":["#Model Definition"]},{"cell_type":"markdown","source":["with lr_scheduler"],"metadata":{"id":"XaOMAtNIhKsv"}},{"cell_type":"code","source":["import time\n","import copy\n","import torch\n","from torch.optim import lr_scheduler\n","\n","def train_model(model, model_name, dataloaders, criterion, optimizer, num_epochs=20, is_inception=False):\n","    since = time.time()\n","\n","    val_acc_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    epochs_without_improvement = 0\n","    early_stopping_epochs = 5\n","\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()\n","            else:\n","                model.eval()\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            for inputs, labels in dataloaders[phase]:\n","                labels = labels.type(torch.LongTensor)\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    if is_inception and phase == 'train':\n","                        outputs, aux_outputs = model(inputs)\n","                        loss1 = criterion(outputs, labels)\n","                        loss2 = criterion(aux_outputs, labels)\n","                        loss = loss1 + 0.4 * loss2\n","                    else:\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","\n","                if epoch_acc > best_acc:\n","                    epochs_without_improvement = 0\n","                    best_acc = epoch_acc\n","                    best_model_wts = copy.deepcopy(model.state_dict())\n","                    if model_name == \"inception\":\n","                        checkpoint_inception = model\n","                        save_checkpoint(checkpoint_inception, \"/content/drive/MyDrive/Saliency Map Research 2023/Models/thorax_inception.pt\")\n","                    elif model_name == \"densenet\":\n","                        checkpoint_dense = model\n","                        save_checkpoint(checkpoint_dense, \"/content/drive/MyDrive/Saliency Map Research 2023/Models/thorax_dense.pt\")\n","                    elif model_name == \"resnet\":\n","                        checkpoint_dense = model\n","                        save_checkpoint(checkpoint_dense, \"/content/drive/MyDrive/Saliency Map Research 2023/Models/thorax_resnet.pt\")\n","                else:\n","                    epochs_without_improvement += 1\n","\n","                if epochs_without_improvement >= early_stopping_epochs:\n","                    print('Early stopping triggered. No improvement in validation accuracy for {} epochs.'.format(early_stopping_epochs))\n","                    time_elapsed = time.time() - since\n","                    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","                    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","                    # Load best model weights\n","                    model.load_state_dict(best_model_wts)\n","                    return model, val_acc_history\n","\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # Load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history\n","\n","##### CHECKPOINT STORAGE\n","def save_checkpoint(state, filename):\n","  print(\"=======> SAVING CHECKPOINT\")\n","  torch.save(state, filename)\n","\n","###### FEATURE EXTRACT\n","def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","##### INITIALIZE\n","def initialize_model(model_name, num_classes,feature_extract):\n","  model_ft = None\n","  input_size = 0\n","\n","  if model_name == \"densenet\":\n","      model_ft = models.densenet121(pretrained=True)\n","      set_parameter_requires_grad(model_ft, feature_extract)\n","      num_ftrs = model_ft.classifier.in_features\n","      model_ft.classifier = nn.Linear(1024, num_classes)\n","      ##### NUM CLASSES ON LAST FC MIGHT NEED FIXING\n","      input_size = 224\n","\n","  elif model_name == \"inception\":\n","      \"\"\" Inception v3\n","      Be careful, expects (299,299) sized images and has auxiliary output\n","      \"\"\"\n","      model_ft = models.inception_v3(pretrained=True)\n","      set_parameter_requires_grad(model_ft, feature_extract)\n","      num_ftrs = model_ft.AuxLogits.fc.in_features\n","      model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","      ##### NUM CLASSES ON LAST FC MIGHT NEED FIXING\n","      num_ftrs = model_ft.fc.in_features\n","      model_ft.fc = nn.Linear(num_ftrs,num_classes)\n","      ##### NUM CLASSES ON LAST FC MIGHT NEED FIXING\n","      input_size = 299\n","\n","  elif model_name == \"resnet\":\n","      \"\"\" Resnet101\n","      \"\"\"\n","      model_ft = models.resnet101(pretrained=True)\n","      set_parameter_requires_grad(model_ft, feature_extract)\n","      num_ftrs = model_ft.fc.in_features\n","      model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","      input_size = 224\n","\n","  else:\n","      print(\"Invalid model name, exiting...\")\n","      exit()\n","\n","  return model_ft, input_size"],"metadata":{"id":"mQ9bjR7sgYei"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CO5d5AUQZZZx"},"outputs":[],"source":["##### Code from pytorch finetuning torchvision models tutorial\n","##### From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n","\n","##### CHECKPOINT STORAGE\n","def save_checkpoint(state, filename):\n","  print(\"=======> SAVING CHECKPOINT\")\n","  torch.save(state, filename)\n","\n","###### FEATURE EXTRACT\n","def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","##### TRAIN FUNCTION\n","def train_model(model, model_name, dataloaders, criterion, optimizer, num_epochs=20, is_inception=False):\n","    since = time.time()\n","\n","    val_acc_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()\n","            else:\n","                model.eval()\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            for inputs, labels in dataloaders[phase]:\n","                labels = labels.type(torch.LongTensor) ####DIFFERENT\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    if is_inception and phase == 'train':\n","                        outputs, aux_outputs = model(inputs)\n","                        loss1 = criterion(outputs, labels)\n","                        loss2 = criterion(aux_outputs, labels)\n","                        loss = loss1 + 0.4*loss2\n","                    else:\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            #epoch_auc = metric(outputs, labels)\n","            #print(outputs, labels)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            ##### SAVE MODEL AND CALC BEST STATS\n","            if phase == 'val' and epoch_acc > best_acc:\n","                if model_name == \"inception\":\n","                  checkpoint_inception = model\n","                  #save_checkpoint(checkpoint_inception, \"/content/drive/MyDrive/Saliency Map Research 2023/Models/thorax_inception.pt\")\n","                  save_checkpoint(checkpoint_inception, \"/content/drive/MyDrive/Saliency Map Research 2023/Repeatability/new model/new_thorax_inception.pt\")\n","                elif model_name == \"densenet\":\n","                  checkpoint_dense = model\n","                  save_checkpoint(checkpoint_dense, \"/content/drive/MyDrive/Saliency Map Research 2023/Models/thorax_dense.pt\")\n","                elif model_name == \"resnet\":\n","                  checkpoint_dense = model\n","                  save_checkpoint(checkpoint_dense, \"/content/drive/MyDrive/Saliency Map Research 2023/Models/thorax_resnet.pt\")\n","                best_acc = epoch_acc\n","                #best_auc = epoch_auc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","\n","            ##### Break Statements for Early Stopping\n","            #if phase == 'val' and model_name == \"inception\" and best_auc > inception_early_stopping:\n","            #  break\n","            #elif phase == 'val' and model_name == \"densenet\" and best_auc > densenet_early_stopping:\n","            #  break\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history\n","\n","##### INITIALIZE\n","def initialize_model(model_name, num_classes,feature_extract):\n","  model_ft = None\n","  input_size = 0\n","\n","  if model_name == \"densenet\":\n","      model_ft = models.densenet121(pretrained=True)\n","      set_parameter_requires_grad(model_ft, feature_extract)\n","      num_ftrs = model_ft.classifier.in_features\n","      model_ft.classifier = nn.Linear(1024, num_classes)\n","      ##### NUM CLASSES ON LAST FC MIGHT NEED FIXING\n","      input_size = 224\n","\n","  elif model_name == \"inception\":\n","      \"\"\" Inception v3\n","      Be careful, expects (299,299) sized images and has auxiliary output\n","      \"\"\"\n","      model_ft = models.inception_v3(pretrained=True)\n","      set_parameter_requires_grad(model_ft, feature_extract)\n","      num_ftrs = model_ft.AuxLogits.fc.in_features\n","      model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","      ##### NUM CLASSES ON LAST FC MIGHT NEED FIXING\n","      num_ftrs = model_ft.fc.in_features\n","      model_ft.fc = nn.Linear(num_ftrs,num_classes)\n","      ##### NUM CLASSES ON LAST FC MIGHT NEED FIXING\n","      input_size = 299\n","\n","  elif model_name == \"resnet\":\n","      \"\"\" Resnet101\n","      \"\"\"\n","      model_ft = models.resnet101(pretrained=True)\n","      set_parameter_requires_grad(model_ft, feature_extract)\n","      num_ftrs = model_ft.fc.in_features\n","      model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","      input_size = 224\n","\n","  else:\n","      print(\"Invalid model name, exiting...\")\n","      exit()\n","\n","  return model_ft, input_size"]},{"cell_type":"markdown","metadata":{"id":"BCn8TXe6yShy"},"source":["# Inception V3 Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbn4GdB0-v1N"},"outputs":[],"source":["##### INPUTS\n","num_classes = 2\n","num_epochs_v3 = 20\n","model_name_v3 = \"resnet\"\n","m = nn.Sigmoid()\n","#criterion = nn.BCEWithLogitsLoss()\n","criterion = nn.CrossEntropyLoss()\n","feature_extract = False\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7KvIMk9o3vS"},"outputs":[],"source":["model_ft, input_size = initialize_model(model_name_v3, num_classes, feature_extract)\n","#print(model_ft)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwL8JHc_pdUL"},"outputs":[],"source":["###### UPDATING MODEL PARAMETERS, GET SUMMARY\n","\n","model_ft = model_ft.to(device)\n","params_to_update = model_ft.parameters()\n","\n","print(\"Params to learn:\")\n","if feature_extract:\n","    params_to_update = []\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            print(\"\\t\",name)\n","else:\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            print(\"\\t\",name)\n","\n","optimizer_ft_v3 = optim.SGD(params_to_update, lr=0.00007, momentum=0.9)\n","#optimizer_ft_v3 = optim.Adam(params_to_update, lr=1e-4)\n","\n","#summary(model_ft, (3, 299, 299))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"osLdQDlupATv"},"outputs":[],"source":["model_ft, hist = train_model(model_ft, model_name_v3, dataloaders_dict_v3, criterion, optimizer_ft_v3, num_epochs=num_epochs_v3, is_inception=(model_name_v3==\"inception\"))"]},{"cell_type":"code","source":["torch.save(model_ft, '/content/drive/MyDrive/Saliency Map Research 2023/Repeatability/new model/new_thorax_inception2.pt')"],"metadata":{"id":"Gd2GMgD44c6h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QtSS6-TRVzYk"},"source":["#Dense-Net 121"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wTFdT0tvaZUb"},"outputs":[],"source":["##### DENSE NET 121 MODEL #####\n","\n","##### DENSE NET NEXT\n","##### POTENTIALLY MOVE TO NEW NOTEBOOK\n","\n","#dense = models.densenet121(pretrained=True)\n","model_name_dense = \"densenet\"\n","#criterion = nn.BCEWithLogitsLoss()\n","num_epochs_dense = 30"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Zp2NsQWaVnF"},"outputs":[],"source":["# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name_dense, num_classes, feature_extract)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FE2f0ZbGSsNp"},"outputs":[],"source":["###### UPDATING MODEL PARAMETERS, GET SUMMARY\n","\n","model_ft = model_ft.to(device)\n","params_to_update = model_ft.parameters()\n","\n","print(\"Params to learn:\")\n","if feature_extract:\n","    params_to_update = []\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            print(\"\\t\",name)\n","else:\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            print(\"\\t\",name)\n","\n","optimizer_ft_dense = optim.SGD(params_to_update, lr=7e-5, momentum=0.9)\n","\n","#summary(model_ft, (3, 299, 299))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPq_y7d0TyOJ"},"outputs":[],"source":["# Train and evaluate\n","model_ft, hist = train_model(model_ft, model_name_dense, dataloaders_dict_dense, criterion, optimizer_ft_dense, num_epochs=num_epochs_dense, is_inception=False)"]},{"cell_type":"markdown","source":["#Test"],"metadata":{"id":"0OlYx_6weIhM"}},{"cell_type":"code","source":["batch_sz = 4\n","\n","val_dataset_v3 = PneumothoraxDataset(root, df_val, transform=transform_v3)\n","val_loader_v3 = torch.utils.data.DataLoader(dataset=val_dataset_v3, batch_size=batch_sz, shuffle=False)\n","test_dataset_v3 = PneumothoraxDataset(root, df_test, transform=transform_v3)\n","test_loader_v3 = torch.utils.data.DataLoader(dataset=test_dataset_v3, batch_size=batch_sz, shuffle=True)\n","\n","val_dataset_dense = PneumothoraxDataset(root, df_val, transform=transform_dense)\n","val_loader_dense = torch.utils.data.DataLoader(dataset=val_dataset_dense, batch_size=batch_sz, shuffle=False)\n","test_dataset_dense = PneumothoraxDataset(root, df_test, transform=transform_dense)\n","test_loader_dense = torch.utils.data.DataLoader(dataset=test_dataset_dense, batch_size=batch_sz, shuffle=True)"],"metadata":{"id":"gAMe70RregSw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import metrics\n","from tqdm import tqdm\n","import torch.nn.functional as nnf"],"metadata":{"id":"_rUSy8JReha_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.autograd import Variable\n","\n","model = model_ft\n","\n","model.eval()\n","prob1 = []\n","results = []\n","final_targets = []\n","#names = []\n","print('===============================================start')\n","for num, data in enumerate(test_loader_dense):\n","     #print(num)\n","     #print(\"=====================================================\")\n","\n","     imgs, label = data\n","     imgs,labels = imgs.to(device), label.to(device)\n","     test = Variable(imgs)\n","     output = model(test)\n","     #print(output)\n","     ps = torch.exp(output)\n","     prob = nnf.softmax(output, dim=1)\n","     prob1 += prob.detach().cpu().numpy().tolist()\n","     #print(prob1[:1])\n","     top_p, top_class = prob.topk(1, dim = 1)\n","     results += top_class.cpu().numpy().tolist()\n","\n","     targets = labels.detach().cpu().numpy().tolist()\n","     final_targets.extend(targets)\n","\n","print('===============================================end')"],"metadata":{"id":"jpL-drD6ekHq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(prob1)\n","prob2 = []\n","for x in prob1:\n","  prob2.append(x[1])\n","\n","print(prob2)\n","\n","#print(final_targets)\n","#results = [item for sublist in results for item in sublist]\n","#print(results)\n","roc_auc = metrics.roc_auc_score(final_targets, results)\n","print(roc_auc)"],"metadata":{"id":"XnqlXYouenok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["roc_auc = metrics.roc_auc_score(final_targets, prob2)\n","print(roc_auc)"],"metadata":{"id":"D3m5IAU8ey7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fpr, tpr, thresh = metrics.roc_curve(final_targets,  prob2, drop_intermediate=False)\n","#create ROC curve\n","plt.plot(fpr,tpr)\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.show()"],"metadata":{"id":"Iygjz1_Fe0nx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gH70OlFxQjEU"},"source":["#ResNet101\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vnRsCcbIQrRE"},"outputs":[],"source":["##### ResNet 101 MODEL #####\n","\n","##### Used for object detection\n","#####\n","\n","resnet = models.resnet101(pretrained=True)\n","model_name_resnet = \"resnet\"\n","#criterion = nn.BCEWithLogitsLoss()\n","num_epochs_resnet = 20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2sqGqnjRq__"},"outputs":[],"source":["# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name_resnet, num_classes, feature_extract)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2JYmV97XDT5"},"outputs":[],"source":["# Send the model to GPU\n","model_ft = model_ft.to(device)\n","params_to_update = model_ft.parameters()\n","optimizer_ft_resnet = optim.Adam(params_to_update, lr=1e-4)\n","\n","#summary(model_ft, (3, 224, 224))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tf5ZD5HQThlE"},"outputs":[],"source":["# Train and evaluate\n","model_ft, hist = train_model(model_ft, model_name_resnet, dataloaders_dict_dense, criterion, optimizer_ft_resnet, num_epochs=num_epochs_resnet, is_inception=False)"]},{"cell_type":"markdown","metadata":{"id":"hnVE2qoU5j72"},"source":["\n","#LOAD MODELS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTsvHOb7b2Fr"},"outputs":[],"source":["inception_load = torch.load('/content/drive/MyDrive/Pneumonia Dataset/Inception_Model/inception.pt')\n","print(inception_load.keys())\n","\n","def load_checkpoint(checkpoint):\n","  model_ft.load_state_dict(checkpoint['state_dict'])\n","\n","inception = load_checkpoint(torch.load(\"/content/drive/MyDrive/Pneumonia Dataset/Inception_Model/inception.pt\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIMkZIUigsR9"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1VAH_-MtgbkBHjDpRLQXb8rFlRqTMAqG2","timestamp":1680561395283}],"gpuClass":"premium","machine_shape":"hm","gpuType":"T4","mount_file_id":"1TIdMX--4jjjBlTWypyHWmecan5ueKjzp","authorship_tag":"ABX9TyO+qqN/6IEEepjwnr0kedsM"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}